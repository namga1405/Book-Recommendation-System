{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [ISBN, Book-Title, Book-Author, Year-Of-Publication, Book-Publisher]\n",
      "Index: []\n",
      "YOF: 0 , Count: 314\n",
      "YOF: 1920 , Count: 2\n",
      "YOF: 1927 , Count: 1\n",
      "YOF: 1929 , Count: 1\n",
      "YOF: 1930 , Count: 1\n",
      "YOF: 1932 , Count: 1\n",
      "YOF: 1936 , Count: 1\n",
      "YOF: 1938 , Count: 1\n",
      "YOF: 1942 , Count: 1\n",
      "YOF: 1945 , Count: 1\n",
      "YOF: 1946 , Count: 2\n",
      "YOF: 1947 , Count: 1\n",
      "YOF: 1949 , Count: 1\n",
      "YOF: 1950 , Count: 2\n",
      "YOF: 1951 , Count: 2\n",
      "YOF: 1952 , Count: 3\n",
      "YOF: 1953 , Count: 14\n",
      "YOF: 1954 , Count: 6\n",
      "YOF: 1955 , Count: 4\n",
      "YOF: 1956 , Count: 5\n",
      "YOF: 1957 , Count: 4\n",
      "YOF: 1958 , Count: 5\n",
      "YOF: 1959 , Count: 6\n",
      "YOF: 1960 , Count: 6\n",
      "YOF: 1961 , Count: 9\n",
      "YOF: 1962 , Count: 6\n",
      "YOF: 1963 , Count: 9\n",
      "YOF: 1964 , Count: 9\n",
      "YOF: 1965 , Count: 7\n",
      "YOF: 1966 , Count: 10\n",
      "YOF: 1967 , Count: 4\n",
      "YOF: 1968 , Count: 5\n",
      "YOF: 1969 , Count: 19\n",
      "YOF: 1970 , Count: 25\n",
      "YOF: 1971 , Count: 15\n",
      "YOF: 1972 , Count: 31\n",
      "YOF: 1973 , Count: 19\n",
      "YOF: 1974 , Count: 24\n",
      "YOF: 1975 , Count: 13\n",
      "YOF: 1976 , Count: 46\n",
      "YOF: 1977 , Count: 60\n",
      "YOF: 1978 , Count: 55\n",
      "YOF: 1979 , Count: 58\n",
      "YOF: 1980 , Count: 61\n",
      "YOF: 1981 , Count: 100\n",
      "YOF: 1982 , Count: 128\n",
      "YOF: 1983 , Count: 182\n",
      "YOF: 1984 , Count: 195\n",
      "YOF: 1985 , Count: 191\n",
      "YOF: 1986 , Count: 216\n",
      "YOF: 1987 , Count: 255\n",
      "YOF: 1988 , Count: 279\n",
      "YOF: 1989 , Count: 394\n",
      "YOF: 1990 , Count: 525\n",
      "YOF: 1991 , Count: 574\n",
      "YOF: 1992 , Count: 590\n",
      "YOF: 1993 , Count: 634\n",
      "YOF: 1994 , Count: 842\n",
      "YOF: 1995 , Count: 935\n",
      "YOF: 1996 , Count: 1012\n",
      "YOF: 1997 , Count: 1064\n",
      "YOF: 1998 , Count: 1136\n",
      "YOF: 1999 , Count: 1346\n",
      "YOF: 2000 , Count: 1384\n",
      "YOF: 2001 , Count: 1572\n",
      "YOF: 2002 , Count: 1750\n",
      "YOF: 2003 , Count: 1530\n",
      "YOF: 2004 , Count: 475\n",
      "YOF: 2005 , Count: 3\n",
      "YOF: 2030 , Count: 3\n",
      "Make: 1920 , Count: 2\n",
      "Make: 1927 , Count: 1\n",
      "Make: 1929 , Count: 1\n",
      "Make: 1930 , Count: 1\n",
      "Make: 1932 , Count: 1\n",
      "Make: 1936 , Count: 1\n",
      "Make: 1938 , Count: 1\n",
      "Make: 1942 , Count: 1\n",
      "Make: 1945 , Count: 1\n",
      "Make: 1946 , Count: 2\n",
      "Make: 1947 , Count: 1\n",
      "Make: 1949 , Count: 1\n",
      "Make: 1950 , Count: 2\n",
      "Make: 1951 , Count: 2\n",
      "Make: 1952 , Count: 3\n",
      "Make: 1953 , Count: 14\n",
      "Make: 1954 , Count: 6\n",
      "Make: 1955 , Count: 4\n",
      "Make: 1956 , Count: 5\n",
      "Make: 1957 , Count: 4\n",
      "Make: 1958 , Count: 5\n",
      "Make: 1959 , Count: 6\n",
      "Make: 1960 , Count: 6\n",
      "Make: 1961 , Count: 9\n",
      "Make: 1962 , Count: 6\n",
      "Make: 1963 , Count: 9\n",
      "Make: 1964 , Count: 9\n",
      "Make: 1965 , Count: 7\n",
      "Make: 1966 , Count: 10\n",
      "Make: 1967 , Count: 4\n",
      "Make: 1968 , Count: 5\n",
      "Make: 1969 , Count: 19\n",
      "Make: 1970 , Count: 25\n",
      "Make: 1971 , Count: 15\n",
      "Make: 1972 , Count: 31\n",
      "Make: 1973 , Count: 19\n",
      "Make: 1974 , Count: 24\n",
      "Make: 1975 , Count: 13\n",
      "Make: 1976 , Count: 46\n",
      "Make: 1977 , Count: 60\n",
      "Make: 1978 , Count: 55\n",
      "Make: 1979 , Count: 58\n",
      "Make: 1980 , Count: 61\n",
      "Make: 1981 , Count: 100\n",
      "Make: 1982 , Count: 128\n",
      "Make: 1983 , Count: 182\n",
      "Make: 1984 , Count: 195\n",
      "Make: 1985 , Count: 191\n",
      "Make: 1986 , Count: 216\n",
      "Make: 1987 , Count: 255\n",
      "Make: 1988 , Count: 279\n",
      "Make: 1989 , Count: 394\n",
      "Make: 1990 , Count: 525\n",
      "Make: 1991 , Count: 574\n",
      "Make: 1992 , Count: 590\n",
      "Make: 1993 , Count: 634\n",
      "Make: 1994 , Count: 842\n",
      "Make: 1995 , Count: 935\n",
      "Make: 1996 , Count: 1012\n",
      "Make: 1997 , Count: 1064\n",
      "Make: 1998 , Count: 1136\n",
      "Make: 1999 , Count: 1346\n",
      "Make: 2000 , Count: 1384\n",
      "Make: 2001 , Count: 1572\n",
      "Make: 2002 , Count: 2067\n",
      "Make: 2003 , Count: 1530\n",
      "Make: 2004 , Count: 475\n",
      "Make: 2005 , Count: 3\n",
      "          ISBN                                         Book-Title  \\\n",
      "0   0002005018                                       clara callan   \n",
      "1   0374157065  flu: the story of the great influenza pandemic...   \n",
      "2   0399135782                             the kitchen god's wife   \n",
      "3   0440234743                                      the testament   \n",
      "4   0452264464               beloved (plume contemporary fiction)   \n",
      "5   0609804618  our dumb century: the onion presents 100 years...   \n",
      "6   1841721522  new vegetarian: bold and beautiful recipes for...   \n",
      "7   0971880107                                        wild animus   \n",
      "8   0345402871                                           airframe   \n",
      "9   0345417623                                           timeline   \n",
      "10  0375759778                                   prague : a novel   \n",
      "11  0375406328                                        lying awake   \n",
      "12  0446310786                              to kill a mockingbird   \n",
      "13  0449005615                     seabiscuit: an american legend   \n",
      "14  0060168013                                     pigs in heaven   \n",
      "15  0061099686                                           downtown   \n",
      "16  0553582909                                           icebound   \n",
      "17  0671888587                                 i'll be seeing you   \n",
      "18  0553582747                         from the corner of his eye   \n",
      "19  0425182908                                       isle of dogs   \n",
      "20  042518630X                                    purity in death   \n",
      "21  0440223571  this year it will be different: and other stories   \n",
      "22  0842342702  left behind: a novel of the earth's last days ...   \n",
      "23  0440225701                                  the street lawyer   \n",
      "24  0060914068                        love, medicine and miracles   \n",
      "25  0380715899                         a soldier of the great war   \n",
      "26  0671623249                                      lonesome dove   \n",
      "27  0679810307     shabanu: daughter of the wind (border trilogy)   \n",
      "28  0679865691                         haveli (laurel leaf books)   \n",
      "29  042511774X                                  breathing lessons   \n",
      "30  0804106304                                  the joy luck club   \n",
      "31  1853260053    tess of the d'urbervilles (wordsworth classics)   \n",
      "32  0060938412                              the accidental virgin   \n",
      "33  0140067477                                    the tao of pooh   \n",
      "34  0345465083                                         seabiscuit   \n",
      "35  1558531025  life's little instruction book (life's little ...   \n",
      "36  0441783589                                  starship troopers   \n",
      "37  0394895894  the ruby in the smoke (sally lockhart trilogy,...   \n",
      "38  0375410538                                       anil's ghost   \n",
      "39  087113375X  modern manners: an etiquette book for rude people   \n",
      "40  0340767936                                     turning thirty   \n",
      "41  0316769487                             the catcher in the rye   \n",
      "42  8445071408  el senor de los anillos: la comunidad del anil...   \n",
      "43  8445071769  el senor de los anillos: las dos torres (lord ...   \n",
      "44  8445071777  el senor de los anillos: el retorno del rey (t...   \n",
      "45  0679429220  midnight in the garden of good and evil: a sav...   \n",
      "46  0671867156                          pretend you don't see her   \n",
      "47  0312252617                                         fast women   \n",
      "48  0312261594                                female intelligence   \n",
      "49  0316748641      pasquale's nose: idle days in an italian town   \n",
      "\n",
      "               Book-Author  Year-Of-Publication  \\\n",
      "0     richard bruce wright                 2001   \n",
      "1         gina bari kolata                 1999   \n",
      "2                  amy tan                 1991   \n",
      "3             john grisham                 1999   \n",
      "4            toni morrison                 1994   \n",
      "5                the onion                 1999   \n",
      "6       celia brooks brown                 2001   \n",
      "7             rich shapero                 2004   \n",
      "8         michael crichton                 1997   \n",
      "9         michael crichton                 2000   \n",
      "10         arthur phillips                 2003   \n",
      "11            mark salzman                 2000   \n",
      "12              harper lee                 1988   \n",
      "13       laura hillenbrand                 2002   \n",
      "14      barbara kingsolver                 1993   \n",
      "15     anne rivers siddons                 1995   \n",
      "16          dean r. koontz                 2000   \n",
      "17      mary higgins clark                 1994   \n",
      "18             dean koontz                 2001   \n",
      "19       patricia cornwell                 2002   \n",
      "20               j.d. robb                 2002   \n",
      "21            maeve binchy                 1997   \n",
      "22              tim lahaye                 2000   \n",
      "23            john grisham                 1999   \n",
      "24   m.d. bernie s. siegel                 1988   \n",
      "25            mark helprin                 1992   \n",
      "26          larry mcmurtry                 1986   \n",
      "27  suzanne fisher staples                 1991   \n",
      "28  suzanne fisher staples                 1995   \n",
      "29              anne tyler                 1994   \n",
      "30                 amy tan                 1994   \n",
      "31            thomas hardy                 1997   \n",
      "32         valerie frankel                 2003   \n",
      "33           benjamin hoff                 1983   \n",
      "34       laura hillenbrand                 2003   \n",
      "35        h. jackson brown                 1991   \n",
      "36      robert a. heinlein                 1987   \n",
      "37          philip pullman                 1988   \n",
      "38        michael ondaatje                 2000   \n",
      "39           p.j. o'rourke                 1990   \n",
      "40              mike gayle                 2000   \n",
      "41           j.d. salinger                 1991   \n",
      "42        j. r. r. tolkien                 2001   \n",
      "43        j. r. r. tolkien                 2001   \n",
      "44        j. r. r. tolkien                 2001   \n",
      "45            john berendt                 1994   \n",
      "46      mary higgins clark                 1998   \n",
      "47         jennifer crusie                 2001   \n",
      "48             jane heller                 2001   \n",
      "49            michael rips                 2002   \n",
      "\n",
      "                         Book-Publisher  \n",
      "0                 harperflamingo canada  \n",
      "1                  farrar straus giroux  \n",
      "2                      putnam pub group  \n",
      "3                                  dell  \n",
      "4                                 plume  \n",
      "5                    three rivers press  \n",
      "6             ryland peters & small ltd  \n",
      "7                               too far  \n",
      "8                      ballantine books  \n",
      "9                      ballantine books  \n",
      "10        random house trade paperbacks  \n",
      "11                      alfred a. knopf  \n",
      "12               little brown & company  \n",
      "13                     ballantine books  \n",
      "14                        harpercollins  \n",
      "15                          harpertorch  \n",
      "16                         bantam books  \n",
      "17                               pocket  \n",
      "18                         bantam books  \n",
      "19             berkley publishing group  \n",
      "20             berkley publishing group  \n",
      "21                                 dell  \n",
      "22             tyndale house publishers  \n",
      "23                                 dell  \n",
      "24             harpercollins publishers  \n",
      "25                           avon books  \n",
      "26                               pocket  \n",
      "27                          laurel leaf  \n",
      "28                          laurel leaf  \n",
      "29             berkley publishing group  \n",
      "30                 prentice hall (k-12)  \n",
      "31  ntc/contemporary publishing company  \n",
      "32                           avon trade  \n",
      "33                        penguin books  \n",
      "34                     ballantine books  \n",
      "35                        thomas nelson  \n",
      "36                            ace books  \n",
      "37                          laurel leaf  \n",
      "38                                knopf  \n",
      "39               atlantic monthly press  \n",
      "40  hodder & stoughton general division  \n",
      "41                        little, brown  \n",
      "42                            minotauro  \n",
      "43                            minotauro  \n",
      "44                          distribooks  \n",
      "45                         random house  \n",
      "46                               pocket  \n",
      "47                   st. martin's press  \n",
      "48                   st. martin's press  \n",
      "49                       back bay books  \n",
      "            ISBN               Book-Title    Book-Author  Year-Of-Publication  \\\n",
      "7399  039592720x  interpreter of maladies  jhumpa lahiri                 1999   \n",
      "\n",
      "           Book-Publisher  \n",
      "7399  houghton mifflin co  \n",
      "Empty DataFrame\n",
      "Columns: [ISBN, Book-Title, Book-Author, Year-Of-Publication, Book-Publisher]\n",
      "Index: []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of              ISBN                                         Book-Title  \\\n",
       "0      0002005018                                       clara callan   \n",
       "1      0374157065  flu: the story of the great influenza pandemic...   \n",
       "2      0399135782                             the kitchen god's wife   \n",
       "3      0440234743                                      the testament   \n",
       "4      0452264464               beloved (plume contemporary fiction)   \n",
       "...           ...                                                ...   \n",
       "18180  0375411615                                         love, etc.   \n",
       "18181  0836227751              the wit and whimsy of mary engelbreit   \n",
       "18182  8433966634                            los detectives salvajes   \n",
       "18183  0330353349                  the ice house (tv tie-in edition)   \n",
       "18184  0394757645  trouble is my business (vintage crime/black li...   \n",
       "\n",
       "                Book-Author  Year-Of-Publication             Book-Publisher  \n",
       "0      richard bruce wright                 2001      harperflamingo canada  \n",
       "1          gina bari kolata                 1999       farrar straus giroux  \n",
       "2                   amy tan                 1991           putnam pub group  \n",
       "3              john grisham                 1999                       dell  \n",
       "4             toni morrison                 1994                      plume  \n",
       "...                     ...                  ...                        ...  \n",
       "18180         julian barnes                 2001            alfred a. knopf  \n",
       "18181       mary engelbreit                 1997  andrews mcmeel publishing  \n",
       "18182        roberto bolano                 2003                   anagrama  \n",
       "18183       minette walters                 1997       mcclelland & stewart  \n",
       "18184      raymond chandler                 1992          vintage books usa  \n",
       "\n",
       "[18185 rows x 5 columns]>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books = pd.read_csv(\"BX-Books.csv\")\n",
    "books.head\n",
    "\n",
    "# Checking for any empty rows\n",
    "empty_rows = books[books.isna().any(axis=1) | books.eq('').any(axis=1)]\n",
    "print(empty_rows)\n",
    "\n",
    "# Checking the Year of Publication first\n",
    "YOF = books[\"Year-Of-Publication\"]\n",
    "values, counts = np.unique(YOF, return_counts=True)\n",
    "for value, count in zip(values, counts):\n",
    "    print(\"YOF:\", value, \", Count:\", count)\n",
    "\n",
    "# Performing Scaling on the Numerical Data\n",
    "mode_YOF = YOF.mode()[0]\n",
    "books[\"Year-Of-Publication\"] = books[\"Year-Of-Publication\"].replace(0, mode_YOF)\n",
    "books[\"Year-Of-Publication\"] = books[\"Year-Of-Publication\"].replace(2030, mode_YOF)\n",
    "YOF = books[\"Year-Of-Publication\"]\n",
    "values, counts = np.unique(YOF, return_counts=True)\n",
    "for value, count in zip(values, counts):\n",
    "    print(\"Make:\", value, \", Count:\", count)\n",
    "\n",
    "# Performing Data Manipulation on worded data to make them all lowercase\n",
    "books[\"Book-Publisher\"] = books[\"Book-Publisher\"].str.lower()\n",
    "books[\"Book-Author\"] = books[\"Book-Author\"].str.lower()\n",
    "books[\"Book-Title\"] = books[\"Book-Title\"].str.lower()\n",
    "print(books.iloc[0:50])\n",
    "\n",
    "# Checking the ISBN and if they are formatted correctly\n",
    "def isbn_check(isbn):\n",
    "    if len(isbn) != 10:\n",
    "        return False\n",
    "    if not isbn[:-1].isdigit():\n",
    "        return False\n",
    "    if not (isbn[-1].isdigit() or isbn[-1] == \"X\"):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "invalid_isbn = books[~books[\"ISBN\"].apply(isbn_check)]\n",
    "print(invalid_isbn)\n",
    "\n",
    "# Performing Data Manipulation on the ISBNs to make them consistent\n",
    "invalid_isbn_rows = books[books[\"ISBN\"].str.contains('x')]\n",
    "books.loc[invalid_isbn_rows.index, \"ISBN\"] = books.loc[invalid_isbn_rows.index, \"ISBN\"].str.upper()\n",
    "\n",
    "invalid_isbn = books[~books[\"ISBN\"].apply(isbn_check)]\n",
    "print(invalid_isbn)\n",
    "\n",
    "# View the head of the dataframe\n",
    "books.head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age Category: adult, Count: 36028\n",
      "Age Category: middle-aged adult, Count: 8580\n",
      "Age Category: old adult, Count: 1898\n",
      "Age Category: adolescent, Count: 1618\n",
      "Age Category: kids, Count: 175\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>User-City</th>\n",
       "      <th>User-State</th>\n",
       "      <th>User-Country</th>\n",
       "      <th>User-Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>timmins</td>\n",
       "      <td>ontario</td>\n",
       "      <td>canada</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>germantown</td>\n",
       "      <td>tennessee</td>\n",
       "      <td>usa</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>albuquerque</td>\n",
       "      <td>new mexico</td>\n",
       "      <td>usa</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>chesapeake</td>\n",
       "      <td>virginia</td>\n",
       "      <td>usa</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>adolescent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>26</td>\n",
       "      <td>bellevue</td>\n",
       "      <td>washington</td>\n",
       "      <td>usa</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>32</td>\n",
       "      <td>portland</td>\n",
       "      <td>oregon</td>\n",
       "      <td>usa</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>39</td>\n",
       "      <td>cary</td>\n",
       "      <td>north carolina</td>\n",
       "      <td>usa</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>42</td>\n",
       "      <td>appleton</td>\n",
       "      <td>wisconsin</td>\n",
       "      <td>usa</td>\n",
       "      <td>adolescent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>44</td>\n",
       "      <td>black mountain</td>\n",
       "      <td>north carolina</td>\n",
       "      <td>usa</td>\n",
       "      <td>middle-aged adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>51</td>\n",
       "      <td>renton</td>\n",
       "      <td>washington</td>\n",
       "      <td>usa</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>53</td>\n",
       "      <td>tacoma</td>\n",
       "      <td>washington</td>\n",
       "      <td>usa</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>56</td>\n",
       "      <td>cheyenne</td>\n",
       "      <td>wyoming</td>\n",
       "      <td>usa</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>69</td>\n",
       "      <td>vancouver</td>\n",
       "      <td>british columbia</td>\n",
       "      <td>canada</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>73</td>\n",
       "      <td>wentzville</td>\n",
       "      <td>missouri</td>\n",
       "      <td>usa</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>75</td>\n",
       "      <td>long beach</td>\n",
       "      <td>california</td>\n",
       "      <td>usa</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>78</td>\n",
       "      <td>oakland</td>\n",
       "      <td>california</td>\n",
       "      <td>usa</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>81</td>\n",
       "      <td>santa cruz</td>\n",
       "      <td>california</td>\n",
       "      <td>usa</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>83</td>\n",
       "      <td>eugene</td>\n",
       "      <td>oregon</td>\n",
       "      <td>usa</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>85</td>\n",
       "      <td>london</td>\n",
       "      <td>-</td>\n",
       "      <td>united kingdom</td>\n",
       "      <td>middle-aged adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>97</td>\n",
       "      <td>mechanicsburg</td>\n",
       "      <td>pennsylvania</td>\n",
       "      <td>usa</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>99</td>\n",
       "      <td>franktown</td>\n",
       "      <td>colorado</td>\n",
       "      <td>usa</td>\n",
       "      <td>middle-aged adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>107</td>\n",
       "      <td>lethbridge</td>\n",
       "      <td>alberta</td>\n",
       "      <td>canada</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>114</td>\n",
       "      <td>ligonier</td>\n",
       "      <td>pennsylvania</td>\n",
       "      <td>usa</td>\n",
       "      <td>middle-aged adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>125</td>\n",
       "      <td>lansing</td>\n",
       "      <td>michigan</td>\n",
       "      <td>usa</td>\n",
       "      <td>middle-aged adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>129</td>\n",
       "      <td>bloomington</td>\n",
       "      <td>minnesota</td>\n",
       "      <td>usa</td>\n",
       "      <td>middle-aged adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>165</td>\n",
       "      <td>olympia</td>\n",
       "      <td>washington</td>\n",
       "      <td>usa</td>\n",
       "      <td>old adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>169</td>\n",
       "      <td>arlington</td>\n",
       "      <td>virginia</td>\n",
       "      <td>usa</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>178</td>\n",
       "      <td>rexford</td>\n",
       "      <td>new york</td>\n",
       "      <td>usa</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>185</td>\n",
       "      <td>baton rouge</td>\n",
       "      <td>louisiana</td>\n",
       "      <td>usa</td>\n",
       "      <td>adolescent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    User-ID       User-City         User-State    User-Country  \\\n",
       "0         8         timmins            ontario          canada   \n",
       "1         9      germantown          tennessee             usa   \n",
       "2        16     albuquerque         new mexico             usa   \n",
       "3        17      chesapeake           virginia             usa   \n",
       "4        19               -                  -               -   \n",
       "5        26        bellevue         washington             usa   \n",
       "6        32        portland             oregon             usa   \n",
       "7        39            cary     north carolina             usa   \n",
       "8        42        appleton          wisconsin             usa   \n",
       "9        44  black mountain     north carolina             usa   \n",
       "10       51          renton         washington             usa   \n",
       "11       53          tacoma         washington             usa   \n",
       "12       56        cheyenne            wyoming             usa   \n",
       "13       69       vancouver   british columbia          canada   \n",
       "14       73      wentzville           missouri             usa   \n",
       "15       75      long beach         california             usa   \n",
       "16       78         oakland         california             usa   \n",
       "17       81      santa cruz         california             usa   \n",
       "18       83          eugene             oregon             usa   \n",
       "19       85          london                  -  united kingdom   \n",
       "20       97   mechanicsburg       pennsylvania             usa   \n",
       "21       99       franktown           colorado             usa   \n",
       "22      107      lethbridge            alberta          canada   \n",
       "23      114        ligonier       pennsylvania             usa   \n",
       "24      125         lansing           michigan             usa   \n",
       "25      129     bloomington          minnesota             usa   \n",
       "26      165         olympia         washington             usa   \n",
       "27      169       arlington           virginia             usa   \n",
       "28      178         rexford           new york             usa   \n",
       "29      185     baton rouge          louisiana             usa   \n",
       "\n",
       "             User-Age  \n",
       "0               adult  \n",
       "1               adult  \n",
       "2               adult  \n",
       "3               adult  \n",
       "4          adolescent  \n",
       "5               adult  \n",
       "6               adult  \n",
       "7               adult  \n",
       "8          adolescent  \n",
       "9   middle-aged adult  \n",
       "10              adult  \n",
       "11              adult  \n",
       "12              adult  \n",
       "13              adult  \n",
       "14              adult  \n",
       "15              adult  \n",
       "16              adult  \n",
       "17              adult  \n",
       "18              adult  \n",
       "19  middle-aged adult  \n",
       "20              adult  \n",
       "21  middle-aged adult  \n",
       "22              adult  \n",
       "23  middle-aged adult  \n",
       "24  middle-aged adult  \n",
       "25  middle-aged adult  \n",
       "26          old adult  \n",
       "27              adult  \n",
       "28              adult  \n",
       "29         adolescent  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# Read the DB\n",
    "users = pd.read_csv(\"BX-Users.csv\")\n",
    "\n",
    "\"\"\" Pre-processing the Age column \"\"\"\n",
    "# Manipulating the elements\n",
    "users['User-Age'] = users['User-Age'].str.rstrip('\"')\n",
    "# Imputate the missing data using the mode age\n",
    "ages = users['User-Age']\n",
    "mode_age = ages.mode()[0]\n",
    "ages = ages.fillna(mode_age).astype(int)\n",
    "\n",
    "# Convert to object dtype\n",
    "ages = ages.astype(object)\n",
    "\n",
    "# Classify the ages into different age categories\n",
    "for i in  range(len(ages)):\n",
    "  if (ages[i] < 10):\n",
    "    ages[i] = 'kids'\n",
    "  elif ((ages[i] >= 10) & (ages[i] < 18)):\n",
    "    ages[i] = 'adolescent'\n",
    "  elif ((ages[i] >= 18) & (ages[i] < 40)):\n",
    "    ages[i] = 'adult'\n",
    "  elif ((ages[i] >= 40) & (ages[i] < 60)):\n",
    "    ages[i] = 'middle-aged adult'\n",
    "  else:\n",
    "    ages[i] = 'old adult'\n",
    "users['User-Age'] = ages\n",
    "# Check to see if all rows have been classified\n",
    "value_counts = users['User-Age'].value_counts()\n",
    "unique_strings = value_counts.index\n",
    "counts = value_counts.values\n",
    "for string, count in zip(unique_strings, counts):\n",
    "    print(f\"Age Category: {string}, Count: {count}\")\n",
    "\n",
    "\n",
    "\"\"\" Pre-processing the Country column \"\"\"\n",
    "# Re-format elements in the column by casefolding \n",
    "users[\"User-Country\"] = users[\"User-Country\"].str.lower().str.strip()\n",
    "# Getting keys from dictionaries of \n",
    "# existing countries from Github repository https://github.com/QuantumTech11/PythonDictionaries/blob/master/Countries%20Dictionary.py \n",
    "Countries = {\n",
    "\"Afghanistan\" : \"Kabul\",\n",
    "\"Albania\" : \"Tirana\",\n",
    "\"Algeria\" : \"Algiers\",\n",
    "\"Andorra\" : \"Andorra la Vella\",\n",
    "\"Angola\" : \"Luanda\",\n",
    "\"Antigua and Barbuda\" : \"Saint John’s\",\n",
    "\"Argentina\" : \"Buenos Aires\",\n",
    "\"Armenia\" : \"Yerevan\",\n",
    "\"Australia\" : \"Canberra\",\n",
    "\"Austria\" : \"Vienna\",\n",
    "\"Azerbaijan\" : \"Baku\",\n",
    "\"The Bahamas\" : \"Nassau\",\n",
    "\"Bahrain\" : \"Manama\",\n",
    "\"Bangladesh\" : \"Dhaka\",\n",
    "\"Barbados\" : \"Bridgetown\",\n",
    "\"Belarus\" : \"Minsk\",\n",
    "\"Belgium\" : \"Brussels\",\n",
    "\"Belize\" : \"Belmopan\",\n",
    "\"Benin\" : \"Porto-Novo\",\n",
    "\"Bhutan\" : \"Thimphu\",\n",
    "\"Bolivia\" : \"La Paz, Sucre\",\n",
    "\"Bosnia and Herzegovina\" : \"Sarajevo\",\n",
    "\"Botswana\" : \"Gaborone\",\n",
    "\"Brazil\" : \"Brasilia\",\n",
    "\"Brunei\" : \"Bandar Seri Begawan\",\n",
    "\"Bulgaria\" : \"Sofia\",\n",
    "\"Burkina Faso\" : \"Ouagadougou\",\n",
    "\"Burundi\" : \"Bujumbura\",\n",
    "\"Cambodia\" : \"Phnom Penh\",\n",
    "\"Cameroon\" : \"Yaounde\",\n",
    "\"Canada\" : \"Ottawa\",\n",
    "\"Cape Verde\" : \"Praia\",\n",
    "\"Central African Republic\" : \"Bangui\",\n",
    "\"Chad\" : \"N’Djamena\",\n",
    "\"Chile\" : \"Santiago\",\n",
    "\"China\" : \"Beijing\",\n",
    "\"Colombia\" : \"Bogota\",\n",
    "\"Comoros\" : \"Moroni\",\n",
    "\"Republic of the Congo\": \"Brazzaville\",\n",
    "\"Democratic Republic of the Congo\" : \"Kinshasa\",\n",
    "\"Costa Rica\" : \"San Jose\",\n",
    "\"Cote d’Ivoire\" : \"Yamoussoukro\",\n",
    "\"Croatia\" : \"Zagreb\",\n",
    "\"Cuba\" : \"Havana\",\n",
    "\"Cyprus\" : \"Nicosia\",\n",
    "\"Czech Republic\" : \"Prague\",\n",
    "\"Denmark\" : \"Copenhagen\",\n",
    "\"Djibouti\" : \"Djibouti\",\n",
    "\"Dominica\" : \"Roseau\",\n",
    "\"Dominican Republic\" : \"Santo Domingo\",\n",
    "\"East Timor\" : \"Dili\",\n",
    "\"Ecuador\" : \"Quito\",\n",
    "\"Egypt\" : \"Cairo\",\n",
    "\"El Salvador\" : \"San Salvador\",\n",
    "\"Equatorial Guinea\" : \"Malabo\",\n",
    "\"Eritrea\" : \"Asmara\",\n",
    "\"Estonia\" : \"Tallinn\",\n",
    "\"Ethiopia\" : \"Addis Ababa\",\n",
    "\"Fiji\" : \"Suva\",\n",
    "\"Finland\" : \"Helsinki\",\n",
    "\"France\" : \"Paris\",\n",
    "\"Gabon\" : \"Libreville\",\n",
    "\"The Gambia\" : \"Banjul\",\n",
    "\"Georgia\" : \"Tbilisi\",\n",
    "\"Germany\" : \"Berlin\",\n",
    "\"Ghana\" : \"Accra\",\n",
    "\"Greece\" : \"Athens\",\n",
    "\"Grenada\" : \"Saint George’s\",\n",
    "\"Guatemala\" : \"Guatemala City\",\n",
    "\"Guinea\" : \"Conakry\",\n",
    "\"Guinea-Bissau\" : \"Bissau\",\n",
    "\"Guyana\" : \"Georgetown\",\n",
    "\"Haiti\" : \"Port-au-Prince\",\n",
    "\"Honduras\" : \"Tegucigalpa\",\n",
    "\"Hungary\" : \"Budapest\",\n",
    "\"Iceland\" : \"Reykjavik\",\n",
    "\"India\" : \"New Delhi\",\n",
    "\"Indonesia\" : \"Jakarta\",\n",
    "\"Iran\" : \"Tehran\",\n",
    "\"Iraq\" : \"Baghdad\",\n",
    "\"Ireland\" : \"Dublin\",\n",
    "\"Israel\" : \"Jerusalem\",\n",
    "\"Italy\" : \"Rome\",\n",
    "\"Jamaica\" : \"Kingston\",\n",
    "\"Japan\" : \"Tokyo\",\n",
    "\"Jordan\" : \"Amman\",\n",
    "\"Kazakhstan\" : \"Astana\",\n",
    "\"Kenya\" : \"Nairobi\",\n",
    "\"Kiribati\" : \"Tarawa Atoll\",\n",
    "\"North Korea\" : \"Pyongyang\",\n",
    "\"South Korea\" : \"Seoul\",\n",
    "\"Kosovo\" : \"Pristina\",\n",
    "\"Kuwait\" : \"Kuwait City\",\n",
    "\"Kyrgyzstan\" : \"Bishkek\",\n",
    "\"Laos\" : \"Vientiane\",\n",
    "\"Latvia\" : \"Riga\",\n",
    "\"Lebanon\" : \"Beirut\",\n",
    "\"Lesotho\" : \"Maseru\",\n",
    "\"Liberia\" : \"Monrovia\",\n",
    "\"Libya\" : \"Tripoli\",\n",
    "\"Liechtenstein\" : \"Vaduz\",\n",
    "\"Lithuania\" : \"Vilnius\",\n",
    "\"Luxembourg\" : \"Luxembourg\",\n",
    "\"Macedonia\" : \"Skopje\",\n",
    "\"Madagascar\" : \"Antananarivo\",\n",
    "\"Malawi\" : \"Lilongwe\",\n",
    "\"Malaysia\" : \"Kuala Lumpur\",\n",
    "\"Maldives\" : \"Male\",\n",
    "\"Mali\" : \"Bamako\",\n",
    "\"Malta\" : \"Valletta\",\n",
    "\"Marshall Islands\" : \"Majuro\",\n",
    "\"Mauritania\" : \"Nouakchott\",\n",
    "\"Mauritius\" : \"Port Louis\",\n",
    "\"Mexico\" : \"Mexico City\",\n",
    "\"Federated States of Micronesia\" : \"Palikir\",\n",
    "\"Moldova\" : \"Chisinau\",\n",
    "\"Monaco\" : \"Monaco\",\n",
    "\"Mongolia\" : \"Ulaanbaatar\",\n",
    "\"Montenegro\" : \"Podgorica\",\n",
    "\"Morocco\" : \"Rabat\",\n",
    "\"Mozambique\" : \"Maputo\",\n",
    "\"Myanmar\" : \"Naypyidaw\",\n",
    "\"Namibia\" : \"Windhoek\",\n",
    "\"Nauru\" : \"Yaren District\",\n",
    "\"Nepal\" : \"Kathmandu\",\n",
    "\"Netherlands\" : \"Amsterdam\",\n",
    "\"New Zealand\" : \"Wellington\",\n",
    "\"Nicaragua\" : \"Managua\",\n",
    "\"Niger\" : \"Niamey\",\n",
    "\"Nigeria\" : \"Abuja\",\n",
    "\"Norway\" : \"Oslo\",\n",
    "\"Oman\" : \"Muscat\",\n",
    "\"Pakistan\" : \"Islamabad\",\n",
    "\"Palau\" : \"Melekeok\",\n",
    "\"Panama\" : \"Panama City\",\n",
    "\"Papua New Guinea\" : \"Port Moresby\",\n",
    "\"Paraguay\" : \"Asuncion\",\n",
    "\"Peru\" : \"Lima\",\n",
    "\"Philippines\" : \"Manila\",\n",
    "\"Poland\" : \"Warsaw\",\n",
    "\"Portugal\" : \"Lisbon\",\n",
    "\"Qatar\" : \"Doha\",\n",
    "\"Romania\" : \"Bucharest\",\n",
    "\"Russia\" : \"Moscow\",\n",
    "\"Rwanda\" : \"Kigali\",\n",
    "\"Saint Kitts and Nevis\" : \"Basseterre\",\n",
    "\"Saint Lucia\" : \"Castries\",\n",
    "\"Saint Vincent and the Grenadines\" : \"Kingstown\",\n",
    "\"Samoa\" : \"Apia\",\n",
    "\"San Marino\" : \"San Marino\",\n",
    "\"Sao Tome and Principe\" : \"Sao Tome\",\n",
    "\"Saudi Arabia\" : \"Riyadh\",\n",
    "\"Senegal\" : \"Dakar\",\n",
    "\"Serbia\" : \"Belgrade\",\n",
    "\"Seychelles\" : \"Victoria\",\n",
    "\"Sierra Leone\" : \"Freetown\",\n",
    "\"Singapore\" : \"Singapore\",\n",
    "\"Slovakia\" : \"Bratislava\",\n",
    "\"Slovenia\" : \"Ljubljana\",\n",
    "\"Solomon Islands\" : \"Honiara\",\n",
    "\"Somalia\" : \"Mogadishu\",\n",
    "\"South Africa\" : \"Pretoria, Cape Town, Bloemfontein\",\n",
    "\"South Sudan\" : \"Juba\",\n",
    "\"Spain\" : \"Madrid\",\n",
    "\"Sri Lanka\" : \"Colombo, Sri Jayewardenepura Kotte\",\n",
    "\"Sudan\" : \"Khartoum\",\n",
    "\"Suriname\" : \"Paramaribo\",\n",
    "\"Swaziland\" : \"Mbabane\",\n",
    "\"Sweden\" : \"Stockholm\",\n",
    "\"Switzerland\" : \"Bern\",\n",
    "\"Syria\" : \"Damascus\",\n",
    "\"Taiwan\" : \"Taipei\",\n",
    "\"Tajikistan\" : \"Dushanbe\",\n",
    "\"Tanzania\" : \"Dodoma\",\n",
    "\"Thailand\" : \"Bangkok\",\n",
    "\"Togo\" : \"Lome\",\n",
    "\"Tonga\" : \"Nuku’alofa\",\n",
    "\"Trinidad and Tobago\" : \"Port-of-Spain\",\n",
    "\"Tunisia\" : \"Tunis\",\n",
    "\"Turkey\" : \"Ankara\",\n",
    "\"Turkmenistan\" : \"Ashgabat\",\n",
    "\"Tuvalu\" : \"Funafuti\",\n",
    "\"Uganda\" : \"Kampala\",\n",
    "\"Ukraine\" : \"Kyiv\",\n",
    "\"United Arab Emirates\" : \"Abu Dhabi\",\n",
    "\"United Kingdom\" : \"London\",\n",
    "\"USA\" : \"Washington D.C.\",\n",
    "\"Uruguay\" : \"Montevideo\",\n",
    "\"Uzbekistan\" : \"Tashkent\",\n",
    "\"Vanuatu\" : \"Port-Vila\",\n",
    "\"Vatican City\" : \"Vatican City\",\n",
    "\"Venezuela\" : \"Caracas\",\n",
    "\"Vietnam\" : \"Hanoi\",\n",
    "\"Yemen\" : \"Sanaa\",\n",
    "\"Zambia\" : \"Lusaka\",\n",
    "\"Zimbabwe\" : \"Harare\"\n",
    "}\n",
    "countries = list(Countries.keys())\n",
    "# Punctuation removal to facilitate change of elements due to spelling mistakes \n",
    "lowercase_countries = [x.lower() for x in countries]\n",
    "for i in range(len(users)):\n",
    "    country = str(users.at[i, 'User-Country'])\n",
    "    country = re.sub(r'[\\\"\\.\\']', '', country)\n",
    "    users.at[i, 'User-Country'] = country\n",
    "# Finding common mistakes in inputs of elements and correcting it (kind of lamentising maybe?)\n",
    "users.loc[users['User-Country'].str.contains('ame|sta', case=False), 'User-Country'] = 'usa'\n",
    "users.loc[users['User-Country'].str.contains('eng|kind|scot|wales|uk', case=False), 'User-Country'] = 'united kingdom'\n",
    "users.loc[users['User-Country'].str.contains('ita', case=False), 'User-Country'] = 'italy'\n",
    "users.loc[users['User-Country'].str.contains('fra', case=False), 'User-Country'] = 'france'\n",
    "# Classify data that are not names of a country and imputate missing data as blank\n",
    "users.loc[~users['User-Country'].isin(lowercase_countries), 'User-Country'] = '-'\n",
    "\n",
    "\"\"\" Pre-process the States column \"\"\"\n",
    "countries_with_states = ['usa', 'nigeria','mexico','india','brazil','germany','malaysia','austria','myanmar','australia','new zealand', 'south sudan','canada']\n",
    "# Classify rows that have countries that don't have a states as a blank\n",
    "users.loc[~users['User-Country'].isin(countries_with_states), 'User-State'] = '-'\n",
    "\n",
    "\"\"\" Pre-process the City column\"\"\"\n",
    "# Classify rows that don't have a country as blank\n",
    "users.loc[users['User-Country'] == '-', 'User-City'] = '-'\n",
    "# Imputate missing rows as blank\n",
    "users.loc[users['User-City'].isna(), 'User-City'] = '-'\n",
    "\n",
    "''' Test print '''\n",
    "users.head(30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [User-ID, ISBN, Book-Rating]\n",
      "Index: []\n",
      "        User-ID        ISBN  Book-Rating\n",
      "72566     95492  039592720x           10\n",
      "106228   138844  039592720x           10\n",
      "140240   187409  039592720x           10\n",
      "155055   209156  039592720x            9\n",
      "Empty DataFrame\n",
      "Columns: [User-ID, ISBN, Book-Rating]\n",
      "Index: []\n",
      "Ratings: 1 , Count: 712\n",
      "Ratings: 2 , Count: 1173\n",
      "Ratings: 3 , Count: 2559\n",
      "Ratings: 4 , Count: 3855\n",
      "Ratings: 5 , Count: 20080\n",
      "Ratings: 6 , Count: 16195\n",
      "Ratings: 7 , Count: 35437\n",
      "Ratings: 8 , Count: 51206\n",
      "Ratings: 9 , Count: 34822\n",
      "Ratings: 10 , Count: 38125\n",
      "Rows: 204164 , Largest ID: 278854\n",
      "Every ID is seemingly valid!\n"
     ]
    }
   ],
   "source": [
    "ratings = pd.read_csv(\"BX-Ratings.csv\")\n",
    "ratings.head\n",
    "\n",
    "# Checking for any empty rows\n",
    "empty_rows = ratings[ratings.isna().any(axis=1) | ratings.eq('').any(axis=1)]\n",
    "print(empty_rows)\n",
    "\n",
    "# Checking the ISBN and if they are formatted correctly\n",
    "def isbn_check(isbn):\n",
    "    if len(isbn) != 10:\n",
    "        return False\n",
    "    if not isbn[:-1].isdigit():\n",
    "        return False\n",
    "    if not (isbn[-1].isdigit() or isbn[-1] == \"X\"):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "invalid_isbn = ratings[~ratings[\"ISBN\"].apply(isbn_check)]\n",
    "print(invalid_isbn)\n",
    "\n",
    "# Performing Data Manipulation on the ISBNs to make them consistent\n",
    "invalid_isbn_rows = ratings[ratings[\"ISBN\"].str.contains('x')]\n",
    "ratings.loc[invalid_isbn_rows.index, \"ISBN\"] = ratings.loc[invalid_isbn_rows.index, \"ISBN\"].str.upper()\n",
    "\n",
    "invalid_isbn = ratings[~ratings[\"ISBN\"].apply(isbn_check)]\n",
    "print(invalid_isbn)\n",
    "\n",
    "# Checking the ratings and seeing if any are unusual\n",
    "Ratings = ratings[\"Book-Rating\"]\n",
    "values, counts = np.unique(Ratings, return_counts=True)\n",
    "for value, count in zip(values, counts):\n",
    "    print(\"Ratings:\", value, \", Count:\", count)\n",
    "\n",
    "# Checking the User-IDs by finding the largest numbered ID first\n",
    "num_rows = len(ratings)\n",
    "max_ID = ratings[\"User-ID\"].max()\n",
    "print(\"Rows:\", num_rows, \", Largest ID:\", max_ID)\n",
    "\n",
    "# Searching for any unusual data entries\n",
    "valid_ID = (ratings[\"User-ID\"] >= 0)\n",
    "num_invalid_ID = num_rows\n",
    "ID_counter = 0\n",
    "for ID in valid_ID:\n",
    "    ID_counter += 1\n",
    "    if ID == False:\n",
    "        print(\"Row:\", ID_counter, \", ID:\", ID)\n",
    "    else:\n",
    "        num_invalid_ID -= 1\n",
    "        if num_invalid_ID == 0:\n",
    "            print(\"Every ID is seemingly valid!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.merge(ratings, books, on='ISBN', how='inner')\n",
    "merged_data = pd.merge(merged_data, users, on='User-ID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Rating</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Book-Publisher</th>\n",
       "      <th>User-City</th>\n",
       "      <th>User-State</th>\n",
       "      <th>User-Country</th>\n",
       "      <th>User-Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>276744</td>\n",
       "      <td>038550120X</td>\n",
       "      <td>7</td>\n",
       "      <td>a painted house</td>\n",
       "      <td>john grisham</td>\n",
       "      <td>2001</td>\n",
       "      <td>doubleday</td>\n",
       "      <td>torrance</td>\n",
       "      <td>california</td>\n",
       "      <td>usa</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>276754</td>\n",
       "      <td>0684867621</td>\n",
       "      <td>8</td>\n",
       "      <td>the girl who loved tom gordon : a novel</td>\n",
       "      <td>stephen king</td>\n",
       "      <td>1999</td>\n",
       "      <td>scribner</td>\n",
       "      <td>alberta beach</td>\n",
       "      <td>alberta</td>\n",
       "      <td>canada</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>276755</td>\n",
       "      <td>0451166892</td>\n",
       "      <td>5</td>\n",
       "      <td>the pillars of the earth</td>\n",
       "      <td>ken follett</td>\n",
       "      <td>1996</td>\n",
       "      <td>signet book</td>\n",
       "      <td>frankfurt am main</td>\n",
       "      <td>hessen</td>\n",
       "      <td>germany</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>276762</td>\n",
       "      <td>0380711524</td>\n",
       "      <td>5</td>\n",
       "      <td>see jane run</td>\n",
       "      <td>joy fielding</td>\n",
       "      <td>1992</td>\n",
       "      <td>avon</td>\n",
       "      <td>duisburg</td>\n",
       "      <td>nordrhein-westfalen</td>\n",
       "      <td>germany</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>276772</td>\n",
       "      <td>0553572369</td>\n",
       "      <td>7</td>\n",
       "      <td>pay dirt (mrs. murphy mysteries (paperback))</td>\n",
       "      <td>rita mae brown</td>\n",
       "      <td>1996</td>\n",
       "      <td>bantam</td>\n",
       "      <td>bonn</td>\n",
       "      <td>nordrhein-westfalen</td>\n",
       "      <td>germany</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User-ID        ISBN  Book-Rating  \\\n",
       "0   276744  038550120X            7   \n",
       "1   276754  0684867621            8   \n",
       "2   276755  0451166892            5   \n",
       "3   276762  0380711524            5   \n",
       "4   276772  0553572369            7   \n",
       "\n",
       "                                     Book-Title     Book-Author  \\\n",
       "0                               a painted house    john grisham   \n",
       "1       the girl who loved tom gordon : a novel    stephen king   \n",
       "2                      the pillars of the earth     ken follett   \n",
       "3                                  see jane run    joy fielding   \n",
       "4  pay dirt (mrs. murphy mysteries (paperback))  rita mae brown   \n",
       "\n",
       "   Year-Of-Publication Book-Publisher          User-City  \\\n",
       "0                 2001      doubleday           torrance   \n",
       "1                 1999       scribner      alberta beach   \n",
       "2                 1996    signet book  frankfurt am main   \n",
       "3                 1992           avon           duisburg   \n",
       "4                 1996         bantam               bonn   \n",
       "\n",
       "             User-State User-Country User-Age  \n",
       "0            california          usa    adult  \n",
       "1               alberta       canada    adult  \n",
       "2                hessen      germany    adult  \n",
       "3   nordrhein-westfalen      germany    adult  \n",
       "4   nordrhein-westfalen      germany    adult  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop unnecessary columns and rename remaining ones for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2077455323.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[141], line 13\u001b[1;36m\u001b[0m\n\u001b[1;33m    merged_data.rename(columns=column_renames, inplace=True\u001b[0m\n\u001b[1;37m                                                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop = ['Book-Publisher', 'User-City', 'User-State']\n",
    "merged_data.drop(columns_to_drop, axis=1, inplace=True)\n",
    "column_renames = {\n",
    "    'User-ID': 'id', \n",
    "    'ISBN': 'isbn',\n",
    "    'Book-Rating': 'rating', \n",
    "    'Book-Author': 'author', \n",
    "    'Year-Of-Publication': 'yop', \n",
    "    'Book-Title': 'title', \n",
    "    'User-Age': 'age', \n",
    "    'User-Country': 'country'\n",
    "}\n",
    "merged_data.rename(columns=column_renames, inplace=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(204242, 8)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train / test sets for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(merged_data, test_size=0.50)\n",
    "\n",
    "# Create integer mappings for user IDs and ISBNs in the training set\n",
    "user_train = {id_: idx for idx, id_ in enumerate(train_data['id'].unique())}\n",
    "isbn_train = {isbn: idx for idx, isbn in enumerate(train_data['isbn'].unique())}\n",
    "\n",
    "# Create integer mappings for user IDs and ISBNs in the testing set\n",
    "user_test = {id_: idx for idx, id_ in enumerate(test_data['id'].unique())}\n",
    "isbn_test = {isbn: idx for idx, isbn in enumerate(test_data['isbn'].unique())}\n",
    "\n",
    "# Apply integer mappings to training and testing data\n",
    "train_data['u_user'] = train_data['id'].map(user_train)\n",
    "train_data['u_isbn'] = train_data['isbn'].map(isbn_train)\n",
    "test_data['u_user'] = test_data['id'].map(user_test)\n",
    "test_data['u_isbn'] = test_data['isbn'].map(isbn_test)\n",
    "\n",
    "\n",
    "# Reduce data to features for collaborative filtering\n",
    "train_data = train_data[['u_user', 'u_isbn', 'rating']]\n",
    "test_data = test_data[['u_user', 'u_isbn', 'rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u_user</th>\n",
       "      <th>u_isbn</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94074</th>\n",
       "      <td>8971</td>\n",
       "      <td>17463</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157863</th>\n",
       "      <td>2387</td>\n",
       "      <td>12604</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42975</th>\n",
       "      <td>12417</td>\n",
       "      <td>5483</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178163</th>\n",
       "      <td>2806</td>\n",
       "      <td>6918</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165378</th>\n",
       "      <td>125</td>\n",
       "      <td>303</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        u_user  u_isbn  rating\n",
       "94074     8971   17463      10\n",
       "157863    2387   12604       7\n",
       "42975    12417    5483       7\n",
       "178163    2806    6918       7\n",
       "165378     125     303       9"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create User-Book Interaction Matrix\n",
    "n_users = train_data['u_user'].nunique()\n",
    "n_books = train_data['u_isbn'].nunique()\n",
    "train_matrix = np.zeros((n_users, n_books))\n",
    "for entry in train_data.itertuples():\n",
    "    train_matrix[entry.u_user, entry.u_isbn] = entry.rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42397, 18179)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Test Interaction Matrix\n",
    "n_users_test = test_data['u_user'].nunique()\n",
    "n_books_test = test_data['u_isbn'].nunique()\n",
    "test_matrix = np.zeros((n_users_test, n_books_test))\n",
    "for entry in test_data.itertuples():\n",
    "    test_matrix[entry.u_user, entry.u_isbn] = entry.rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find similarity betwen users and items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarity matrices for collaborative filtering through cosine similarity\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "train_matrix_small = train_matrix[:5000, :5000]\n",
    "test_matrix_small = test_matrix[:5000, :5000]\n",
    "user_similarity = pairwise_distances(train_matrix_small, metric='cosine')\n",
    "item_similarity = pairwise_distances(train_matrix_small.T, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict ratings\n",
    "def predict_books(ratings, similarity, type='user'):\n",
    "    if type == 'user':\n",
    "        mean_user_rating = ratings.mean(axis=1)\n",
    "        ratings_diff = (ratings - mean_user_rating[:, np.newaxis])\n",
    "        pred = mean_user_rating[:, np.newaxis] + similarity.dot(ratings_diff) / np.array([np.abs(similarity).sum(axis=1)]).T\n",
    "    else: \n",
    "        pred = ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_prediction = predict_books(train_matrix_small, item_similarity, type='item')\n",
    "user_prediction = predict_books(train_matrix_small, user_similarity, type='user')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Collaborative Filtering using RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item-based CF RMSE: 7.997508952927154\n",
      "User-based CF RMSE: 7.996410502723139\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "def rmse(prediction, ground_truth):\n",
    "    prediction = prediction[ground_truth.nonzero()].flatten()\n",
    "    ground_truth = ground_truth[ground_truth.nonzero()].flatten()\n",
    "    return sqrt(mean_squared_error(prediction, ground_truth))\n",
    "\n",
    "print(f'Item-based CF RMSE: {rmse(item_prediction, test_matrix_small)}')\n",
    "print(f'User-based CF RMSE: {rmse(user_prediction, test_matrix_small)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    1.6098  1.6132  1.6213  1.6124  1.6188  1.6151  0.0043  \n",
      "Fit time          1.95    2.05    2.15    1.99    2.00    2.03    0.07    \n",
      "Test time         0.20    0.20    0.22    0.18    0.18    0.19    0.02    \n",
      "CPU times: total: 13.1 s\n",
      "Wall time: 13.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x28f03349b20>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise import Reader, Dataset\n",
    "from surprise import SVD, model_selection, accuracy\n",
    "\n",
    "# Creating a 'Reader' object to set the limit of the ratings \n",
    "reader = Reader(rating_scale=(1, 10))\n",
    "data = Dataset.load_from_df(ratings, reader)\n",
    "\n",
    "model = SVD()\n",
    "\n",
    "# Train on books dataset\n",
    "%time model_selection.cross_validate(model, data, measures=['RMSE'], cv=5, verbose=True)\n",
    "\n",
    "# train and test split\n",
    "trainset, testset = model_selection.train_test_split(data, test_size=0.50)\n",
    "\n",
    "# SVD model\n",
    "model = SVD()\n",
    "model.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.6162\n",
      "The accuracy is 1.6162345522605832\n"
     ]
    }
   ],
   "source": [
    "# Display RMSE results\n",
    "predictions = model.test(testset)\n",
    "print(f\"The accuracy is {accuracy.rmse(predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: 276744     item: 038550120X r_ui = None   est = 7.31   {'was_impossible': False}\n"
     ]
    }
   ],
   "source": [
    "# Run a test case\n",
    "uid = 276744  \n",
    "iid = '038550120X' \n",
    "pred = model.predict(uid, iid, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>isbn</th>\n",
       "      <th>n__bookrating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6799</th>\n",
       "      <td>66942</td>\n",
       "      <td>0754022471</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17088</th>\n",
       "      <td>170513</td>\n",
       "      <td>0590440691</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11666</th>\n",
       "      <td>110267</td>\n",
       "      <td>0441008585</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22083</th>\n",
       "      <td>225886</td>\n",
       "      <td>0886778115</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>8245</td>\n",
       "      <td>0761119396</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id        isbn  n__bookrating\n",
       "6799    66942  0754022471              7\n",
       "17088  170513  0590440691              5\n",
       "11666  110267  0441008585              7\n",
       "22083  225886  0886778115              6\n",
       "819      8245  0761119396              9"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_ratings = pd.read_csv(\"BX-NewBooksRatings.csv\")\n",
    "n_ratings = n_ratings.rename(columns={'User-ID': 'id', 'ISBN': 'isbn', 'Book-Rating': 'n__bookrating'})\n",
    "n_ratings= n_ratings.sample(frac=0.50)\n",
    "n_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>isbn</th>\n",
       "      <th>book_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>276744</td>\n",
       "      <td>038550120X</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>276754</td>\n",
       "      <td>0684867621</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>276755</td>\n",
       "      <td>0451166892</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>276762</td>\n",
       "      <td>0380711524</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>276772</td>\n",
       "      <td>0553572369</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id        isbn  book_rating\n",
       "0   276744  038550120X            7\n",
       "1   276754  0684867621            8\n",
       "2   276755  0451166892            5\n",
       "3   276762  0380711524            5\n",
       "4   276772  0553572369            7"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = ratings.rename(columns={'User-ID': 'id', 'ISBN': 'isbn', 'Book-Rating': 'book_rating'})\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted rating for ISBN 038550120X from user #276744 is 7.31.\n",
      "\n",
      "The actual rating given was 7.00.\n"
     ]
    }
   ],
   "source": [
    "# Display predicted and actual ratings\n",
    "print(f'Predicted rating for ISBN {pred.iid} from user {pred.uid} is {pred.est:.2f}.\\n')\n",
    "actual_rtg= ratings[(ratings.user_id==pred.uid) & (ratings.isbn==pred.iid)].book_rating.values[0]\n",
    "print(f'The actual rating given was {actual_rtg:.2f}.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COMP20008",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
